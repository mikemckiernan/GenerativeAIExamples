{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ca3af10",
   "metadata": {},
   "source": [
    "## Notebook 8-Option(1): Plugin NVIDIA AI endpoint's [mixtral_8x7b](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/mixtral-8x7b)   into LlamaIndex and Langchain\n",
    "\n",
    "This notebook demonstrates how to plug in a NVIDIA AI Endpoint [mixtral_8x7b](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/mixtral-8x7b) and [embedding nvolveqa_40k](https://python.langchain.com/docs/integrations/text_embedding/nvidia_ai_endpoints#setup), bind these into [LlamaIndex](https://gpt-index.readthedocs.io/en/stable/) with these customizations.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "⚠️ There are continous development and retrieval techniques supported in LlamaIndex and this notebook just shows to quikcly replace components such as llm and embedding to a user-choice, read more [documentation on llama-index](https://docs.llamaindex.ai/en/stable/) for the latest information. \n",
    "</div>\n",
    "\n",
    "### Prerequisite \n",
    "In order to successfully run this notebook, you will need the following -\n",
    "\n",
    "1. Already successfully gone through the [setup](https://python.langchain.com/docs/integrations/text_embedding/nvidia_ai_endpoints#setup) and generated an API key.\n",
    "\n",
    "2. Please verify you have successfully pip install all python packages in [requirements.txt](https://github.com/NVIDIA/GenerativeAIExamples/blob/3d29acf677466c5c301370cab5867cb09e04e318/notebooks/requirements.txt)\n",
    "\n",
    "In this notebook, we will cover the following custom plug-in components -\n",
    "\n",
    "    - LLM using NVIDIA AI Endpoint mixtral_8x7b\n",
    "    \n",
    "    - A NVIDIA AI endpoint embedding nvolveqa_40k\n",
    "    \n",
    "Note: As one can see, since we are using NVIDIA AI endpoints as an API, there is no further requirement in the prerequisites about GPUs as compute hardware\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec37945e",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 1 - Load NVIDIA AI Endpoint [mixtral_8x7b](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/mixtral-8x7b)\n",
    "\n",
    "Note: check the prerequisite if you have not yet obtain a valid API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d20c4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "## API Key can be found by going to NVIDIA NGC -> AI Foundation Models -> (some model) -> Get API Code or similar.\n",
    "## 10K free queries to any endpoint (which is a lot actually).\n",
    "\n",
    "# del os.environ['NVIDIA_API_KEY']  ## delete key and reset\n",
    "if os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    print(\"Valid NVIDIA_API_KEY already in environment. Delete to reset\")\n",
    "else:\n",
    "    nvapi_key = getpass.getpass(\"NVAPI Key (starts with nvapi-): \")\n",
    "    assert nvapi_key.startswith(\"nvapi-\"), f\"{nvapi_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvapi_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3795a45a",
   "metadata": {},
   "source": [
    "run a test and see the model generating output response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c45589e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test run and see that you can genreate a respond successfully \n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "llm = ChatNVIDIA(model=\"mixtral_8x7b\", nvidia_api_key=nvapi_key)\n",
    "result = llm.invoke(\"Write a ballad about LangChain.\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf9bdf0",
   "metadata": {},
   "source": [
    "### Step 2 - Load the chosen NVIDIA Endpoint Embedding into llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c22821a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and dl embeddings instance wrapping huggingface embedding into langchain embedding\n",
    "# Bring in embeddings wrapper\n",
    "from llama_index.embeddings import LangchainEmbedding\n",
    "\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "nv_embedding = NVIDIAEmbeddings(model=\"nvolveqa_40k\", model_type=\"query\")\n",
    "li_embedding=LangchainEmbedding(nv_embedding)\n",
    "# Alternatively, if you want to specify whether it will use the query or passage type\n",
    "# embedder = NVIDIAEmbeddings(model=\"nvolveqa_40k\", model_type=\"passage\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc32ea8b",
   "metadata": {},
   "source": [
    "Note: if you encounter typing_extension error, simply reinstall via :pip install typing_extensions==4.7.1 --force-reinstall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb1b26d",
   "metadata": {},
   "source": [
    "### Step 3 - Wrap the NVIDIA embedding endpoint and the NVIDIA mixtral_8x7b endpoints into llama-index's ServiceContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a565bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in stuff to change service context\n",
    "from llama_index import set_global_service_context\n",
    "from llama_index import ServiceContext\n",
    "\n",
    "# Create new service context instance\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    chunk_size=1024,\n",
    "    llm=llm,\n",
    "    embed_model=li_embedding\n",
    ")\n",
    "# And set the service context\n",
    "set_global_service_context(service_context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9716f425",
   "metadata": {},
   "source": [
    "### Step 4a - Load the text data using llama-index's SimpleDirectoryReader and we will be using the built-in [VectorStoreIndex](https://docs.llamaindex.ai/en/latest/community/integrations/vector_stores.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a756c9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create query engine with cross encoder reranker\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "import torch\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./toy_data\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded9af4b",
   "metadata": {},
   "source": [
    "### Step 4b - This will serve as the query engine for us to ask questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3e0395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup index query engine using LLM \n",
    "query_engine = index.as_query_engine()\n",
    "# Test out a query in natural\n",
    "response = query_engine.query(\"who is the director of the movie Titanic?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b095256b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c09d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95be66c4",
   "metadata": {},
   "source": [
    "--- \n",
    "### Step 5a- One can also use [GPTKnowledgeGraphIndex](https://docs.llamaindex.ai/en/stable/examples/index_structs/knowledge_graph/KnowledgeGraphDemo.html), let's fetch a PDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6898495e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://nvdam.widen.net/content/vuzumiozpb/original/h100-datasheet-2287922.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0479206",
   "metadata": {},
   "source": [
    "### Step 5b- Reading in documents from directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e819b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(input_files=[\"h100-datasheet-2287922.pdf\"]).load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dc8c2b",
   "metadata": {},
   "source": [
    "### Step 5c - We will warp our llm into a service_context\n",
    "\n",
    "If you get time out issue for the API calls, then choose a smaller PDD file with less pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50362f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.callbacks import CallbackManager, LlamaDebugHandler\n",
    "from llama_index import ServiceContext, LLMPredictor\n",
    "\n",
    "llm_predictor = LLMPredictor(llm=llm)\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n",
    "\n",
    "llama_debug = LlamaDebugHandler(print_trace_on_end=True)\n",
    "callback_manager = CallbackManager([llama_debug])\n",
    "service_context = ServiceContext.from_defaults(callback_manager=callback_manager, llm_predictor=llm_predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79879c67",
   "metadata": {},
   "source": [
    "### Step 5d - now we are ready to query using the [GPTKnowledgeGraphIndex](https://docs.llamaindex.ai/en/stable/examples/index_structs/knowledge_graph/KnowledgeGraphDemo.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1078af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.knowledge_graph import GPTKnowledgeGraphIndex\n",
    "index = GPTKnowledgeGraphIndex.from_documents(documents, service_context=service_context)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712b746f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What is H100 performance?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e960932",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
