{
    "cells": [
     {
      "cell_type": "markdown",
      "id": "b4268da6-98d2-4e23-a984-ce49c70d6a42",
      "metadata": {},
      "source": [
       "# Notebook 1: LLM Streaming Client\n",
       "This notebook demonstrates how to stream responses from the LLM. \n",
       "\n",
       "### NeMo Microservice Inference Server\n",
       "The LLM has been deployed to [NVIDIA NeMo Microservice Inference Server](https://registry.ngc.nvidia.com/orgs/ohlfw0olaadg/teams/ea-participants/containers/nemollm-inference-ms) and leverages NVIDIA TensorRT-LLM (TRT-LLM), so it's optimized for low latency and high throughput inference. \n",
       "\n",
       "The **NeMo Microservice Inference** is used to communicate with the inference server hosting the LLM over the REST API. \n",
       "\n",
       "### Streaming LLM Responses\n",
       "TRT-LLM on its own can provide drastic improvements to LLM response latency, but streaming can take the user-experience to the next level. Instead of waiting for an entire response to be returned from the LLM, chunks of it can be processed as soon as they are available. This helps reduce the perceived latency by the user. "
      ]
     },
     {
      "cell_type": "markdown",
      "id": "667181db-04d8-4c9d-b433-26c2a14d54e7",
      "metadata": {},
      "source": [
       "### Step 1: Structure the Query in a Prompt Template"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "7e206005-d153-49ce-8b54-41e425a7de17",
      "metadata": {},
      "source": [
       "A [**prompt template**](https://gpt-index.readthedocs.io/en/stable/api_reference/prompts.html) is a common paradigm in LLM development. \n",
       "\n",
       "They are a pre-defined set of instructions provided to the LLM and guide the output produced by the model. They can contain few shot examples and guidance and are a quick way to engineer the responses from the LLM. Llama 2 accepts the [prompt format](https://huggingface.co/blog/llama2#how-to-prompt-llama-2) shown in `PROMPT_TEMPLATE`, which we modify to be constructed with:\n",
       "- The system prompt\n",
       "- The context\n",
       "- The user's question"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "42a2f2cb",
      "metadata": {},
      "outputs": [],
      "source": [
       "PROMPT_TEMPLATE = (\n",
       " \"<s>[INST] <<SYS>>\"\n",
       " \"{system_prompt}\"\n",
       " \"<</SYS>>\"\n",
       " \"[/INST] {context} </s><s>[INST] {question} [/INST]\"\n",
       ")\n",
       "# For nemotron model uncomment below prompt - prompts are model dependent and response vary depends on prompt\n",
       "# PROMPT_TEMPLATE = (\n",
       "#     \"<extra_id_0>System\"\n",
       "#     \"You are a helpful, respectful and honest assistant.Always answer as helpfully as possible, while being safe. Please ensure that your responses are positive in nature.\"\n",
       "#     \"<extra_id_0>System\"\n",
       "#     \"{context} \\n {question} Given context followed by query, you try to answer the query truthfully\"\n",
       "#     \"<extra_id_1>Assistant\"\n",
       "# )\n",
       "system_prompt = \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Please ensure that your responses are positive in nature.\"\n",
       "context=\"\"\n",
       "question='What is the fastest land animal?'\n",
       "prompt = PROMPT_TEMPLATE.format(system_prompt=system_prompt, context=context, question=question)"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "9e975c7b-3c5e-4ba6-954a-0064e6a91245",
      "metadata": {},
      "source": [
       "### Step 2: Create the Triton Client"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "eec1fda2-e974-4c7d-b656-0b81e452cefb",
      "metadata": {},
      "source": [
       "Additional inputs to the LLM can be modified:\n",
       "- [frequency_penalty](https://platform.openai.com/docs/guides/text-generation/parameter-details): reduce the likelihood of sampling repetitive sequences of tokens\n",
       "- n: [1]: number of alternative text completions or choices to generate\n",
       "- model: model name used for inference\n",
       "- max_tokens: the maximum number of tokens (words/sub-words) generated\n",
       "- stream: enable streaming\n",
       "- temperature: [0,1] -- higher values produce more diverse outputs\n",
       "- stop: specifies a list of stop tokens that signal the end of a response\n",
       "- [top_p](https://docs.cohere.com/docs/controlling-generation-with-top-k-top-p): [0, 1] -- cumulative probability cutoff for token selection; lower values mean sampling from a smaller nucleus sample (reduces variety)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "6907e68d-8ab9-4f94-b6a0-825446261bfc",
      "metadata": {},
      "outputs": [],
      "source": [
       "# If you've changed `MODEL_NAME` in compose.env, update model param with same name in pload\n",
       "pload = {\n",
       "            \"prompt\": prompt,\n",
       "            \"frequency_penalty\": 0,\n",
       "            \"n\": 1,\n",
       "            \"model\": \"Llama-2-13b-chat-hf\",\n",
       "            \"max_tokens\": 300,\n",
       "            \"stream\": True,\n",
       "            \"temperature\":1.0,\n",
       "            \"stop\": [\"</s>\", \"<extra_id_1>\"],\n",
       "}"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "c526b20b-258a-4eb7-87e6-5430d57e32ea",
      "metadata": {},
      "source": [
       "### Step 3: Generate response from NeMo Microservice Inference Server.\n",
       "The NeMo Microservice Inference Server hosts a REST API server with a schema similar to openai. To generate a response, you'll need to send a request to the NeMo Microservice Inference Server URL and receive the generated text.\n",
       "\n"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "c2858c80-ba39-43be-9978-0f8be6e6c3dd",
      "metadata": {},
      "source": [
       "<div class=\"alert alert-block alert-warning\">\n",
       "<b>WARNING!</b> Be sure to replace `triton_url` with the address and port that Triton is running on. \n",
       "</div>\n",
       "\n",
       "Use the address and port that the Triton is available on; for example `localhost:9999`. \n",
       "\n",
       "**If you are running this notebook as part of the AI workflow, you dont have to replace the url**."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "adebd0a6-efa5-47c8-9c5e-45024246b3d5",
      "metadata": {},
      "outputs": [],
      "source": [
       "import requests\n",
       "import json\n",
       "import time\n",
       "\n",
       "tokens_generated = 0\n",
       "start_time = time.time()\n",
       "\n",
       "server_url = \"http://llm:9999/v1/completions\"\n",
       "response = requests.post(server_url, json=pload, stream=True)\n",
       "\n",
       "current_string = \"\"\n",
       "if response.status_code == 200:\n",
       "    for chunk in response.iter_lines():\n",
       "        chunk = chunk.decode(\"utf-8\")\n",
       "        if chunk:\n",
       "            # data: is appended before every chunk, remove it to parse json\n",
       "            chunk = chunk.lstrip(\"data: \")\n",
       "            tokens_generated += 1\n",
       "            try:\n",
       "                # extract text from response\n",
       "                chunk = json.loads(chunk)\n",
       "                chunk = chunk.get(\"choices\", [{}])[0].get(\"text\", \"\")\n",
       "            except Exception as e:\n",
       "                # Non json data with [DONE] represent end of stream\n",
       "                chunk = \"\"\n",
       "\n",
       "            # Unlike openai nemo micorinference inference server returns complete response instead of generated token\n",
       "            # find new generated chunk and send it for streaming\n",
       "            resp = chunk[len(current_string) :]\n",
       "            print(resp, end=\"\", flush=True)\n",
       "            current_string = chunk\n",
       "\n",
       "total_time = time.time() - start_time\n",
       "print(f\"\\n--- Generated {tokens_generated} tokens in {total_time} seconds ---\")\n",
       "print(f\"--- {tokens_generated/total_time} tokens/sec\")"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "fe8e60dd-ec3a-41be-bbaa-63a7971f1e70",
      "metadata": {},
      "source": [
       "### Step 4: Use Nemo Microserivice Inference Langchain wrapper to stream output using llm.\n",
       "We establishes a connection to the Nemo Microservice Inference server running the TRT-LLM Llama-2 model. It utilizes the `NemoInfer` class which is a langchain wrapper for llm from the `nemo_infer`.\n",
       "\n",
       "* `server_url`: The URL of the Nemo Microservice Inference server. Change `server_url` to where nemo inference ms is running. If you're running it as part of generative AI Workflow, you don't have to replace the llm url \n",
       "\n",
       "* `model`: The name of the model to use, which in this case is \"Llama-2-13b-chat-hf\".\n",
       "\n",
       "* `callbacks`: A list of callbacks to be used during inference. The `streaming_stdout.StreamingStdOutCallbackHandler()` callback is used to print the streaming response to the console.\n",
       "\n",
       "* `tokens`: The maximum number of tokens to generate.\n",
       "\n",
       "The `llm` object represents the established connection to the Nemo Microservice Inference server and can be used to generate text using the Llama-2-13b-chat-hf model.\n",
       "\n"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "47fc4138-9d1d-470b-9085-9161dbddedd2",
      "metadata": {},
      "outputs": [],
      "source": [
       "from nemo_infer import NemoInfer\n",
       "from langchain.callbacks import streaming_stdout\n",
       "\n",
       "callbacks = [streaming_stdout.StreamingStdOutCallbackHandler()]\n",
       "# Connect to the TRT-LLM Llama-2 model running on the Nemo Microservice Inference server at the url below\n",
       "llm = NemoInfer(server_url =\"http://llm:9999/v1/completions\", model=\"Llama-2-13b-chat-hf\", callbacks=callbacks, tokens=500)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a535451-7939-4612-94ca-256fda46b3b8",
      "metadata": {},
      "outputs": [],
      "source": [
       "llm(\"Who is CEO of nvidia\")"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 5
   }
