## Configuration Guide

### Chain Server Configuration

In this section, we explore the configurations for the [Chain Server](./chat_server.md) used for the default canonical developer rag example.

Chain server interaction with other components can be controlled by config. Chain Server interacts with components such as the `milvus` vector store and `triton` server, which hosts the Large Language Model (LLM). Additionally, we'll delve into customization options to fine-tune the behavior of the query server. These options include settings for the embedding model, chunk size, and prompts for generating responses. These configurations can be controlled using environment variables in the docker compose files.

### Configuring docker compose file for default RAG example
In this section, we will look into the environment variables and parameters that can be configured within the [Docker Compose](../../deploy/compose/docker-compose.yaml) YAML file for the default canonical example. Our system comprises multiple microservices that interact harmoniously to generate responses. These microservices include LLM Inference Server, Jupyter Server, Milvus, Query/chain server, and Frontend.

#### LLM server Configurations
The LLM Inference Server is used for hosting the Large Language Model (LLM) with triton backend. You can configure the model information using the [compose.env](../../deploy/compose/compose.env) file or by setting the corresponding environment variables. Here is a list of environment variables utilized by the llm inference server:

    MODEL_DIRECTORY: Specifies the path to the model directory where model checkpoints are stored.
    MODEL_ARCHITECTURE: Defines the architecture of the model used for deployment.
    MODEL_MAX_INPUT_LENGTH: Maximum allowed input length, with a default value of 3000.
    MODEL_MAX_OUTPUT_LENGTH: Maximum allowed output lenght, with a default of 512.
    INFERENCE_GPU_COUNT: Specifies the GPUs to be used by Triton for model deployment, with the default setting being "all."

#### Jupyter Server
This server hosts jupyter lab server. This contains notebook explaining the flow of chain server.

#### Milvus
Milvus serves as a GPU-accelerated vector store database, where we store embeddings generated by the knowledge base.

#### Query/Chain Server
The Query service is the core component responsible for interacting with the llm inference server and the Milvus server to obtain responses. The environment variables utilized by this container are described as follows:

    APP_MILVUS_URL: Specifies the URL where the Milvus server is hosted.
    APP_LLM_SERVERURL: Specifies the URL where the Triton server is hosted.
    APP_LLM_MODELNAME: The model name used by the Triton server.
    APP_LLM_MODELENGINE: An enum specifying the backend name hosting the model. Options currently supported are:
    1. `triton-trt-llm` if you are using locally deployed LLM models.
    2. `nv-ai-foundation` if you are using NV AI Playground based models.
    APP_CONFIG_FILE: Provides the path to the configuration file used by the Chain Server or this container. Defaults to /dev/null
    APP_RETRIEVER_TOPK: Number of relevant results to retrieve. Default value is 4.
    APP_RETRIEVER_SCORETHRESHOLD: The minimum confidence score for the retrieved values to be considered. Default value is 0.25.

#### Frontend
The Frontend component is the UI server that interacts with the Query/Chain Server to retrieve responses and provide UI interface to ingest documents. The following environment variables are used by the frontend:

    APP_SERVERURL: Indicates the URL where the Query/Chain Server is hosted.
    APP_SERVERPORT: Specifies the port on which the Query/Chain Server operates.
    APP_MODELNAME: Name of the Large Language Model utilized for deployment. This information is for display purposes only and does not affect the inference process.
