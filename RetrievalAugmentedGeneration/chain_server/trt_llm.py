"""A Langchain LLM component for connecting to Triton + TensorRT LLM backend."""
# pylint: disable=too-many-lines;
# pylint: disable=import-error  # Tests should pass without all the deps installed
import json
import queue
import random
import time
from functools import partial
from typing import Any, Dict, List, Optional, Union

import google.protobuf.json_format
import numpy as np
import tritonclient.grpc as grpcclient
from langchain.callbacks.manager import CallbackManagerForLLMRun
from langchain.llms.base import LLM
from langchain.pydantic_v1 import Field, root_validator
from tritonclient.grpc.service_pb2 import ModelInferResponse
from tritonclient.utils import np_to_triton_dtype

STOP_WORDS = ["</s>"]
RANDOM_SEED = 0


# pylint: disable-next=too-few-public-methods  # Interface is defined by LangChain
class TensorRTLLM(LLM):  # LLM class not typed in langchain
    """A custom Langchain LLM class that integrates with TRTLLM triton models.

    Arguments:
    server_url: (str) The URL of the Triton inference server to use.
    model_name: (str) The name of the Triton TRT model to use.
    temperature: (str) Temperature to use for sampling
    top_p: (float) The top-p value to use for sampling
    top_k: (float) The top k values use for sampling
    beam_width: (int) Last n number of tokens to penalize
    repetition_penalty: (int) Last n number of tokens to penalize
    length_penalty: (float) The penalty to apply repeated tokens
    tokens: (int) The maximum number of tokens to generate.
    client: The client object used to communicate with the inference server
    """

    server_url: str = Field(None, alias="server_url")

    # # all the optional arguments
    model_name: str = "ensemble"
    temperature: Optional[float] = 1.0
    top_p: Optional[float] = 0
    top_k: Optional[int] = 1
    tokens: Optional[int] = 100
    beam_width: Optional[int] = 1
    repetition_penalty: Optional[float] = 1.0
    length_penalty: Optional[float] = 1.0
    client: Any
    streaming: Optional[bool] = True

    @root_validator()  # typing not declared in langchain
    @classmethod
    def validate_environment(cls, values: Dict[str, Any]) -> Dict[str, Any]:
        """Validate that python package exists in environment."""
        try:
            values["client"] = GrpcTritonClient(values["server_url"])

        except ImportError as err:
            raise ImportError(
                "Could not import triton client python package. "
                "Please install it with `pip install tritonclient[all]`."
            ) from err
        return values

    @property
    def _get_model_default_parameters(self) -> Dict[str, Any]:
        return {
            "tokens": self.tokens,
            "top_k": self.top_k,
            "top_p": self.top_p,
            "temperature": self.temperature,
            "repetition_penalty": self.repetition_penalty,
            "length_penalty": self.length_penalty,
            "beam_width": self.beam_width,
        }

    @property
    def _invocation_params(self, **kwargs: Any) -> Dict[str, Any]:
        params = {**self._get_model_default_parameters, **kwargs}
        return params

    @property
    def _identifying_params(self) -> Dict[str, Any]:
        """Get all the identifying parameters."""
        return {
            "server_url": self.server_url,
            "model_name": self.model_name,
        }

    @property
    def _llm_type(self) -> str:
        return "triton_tensorrt"

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,  # pylint: disable=unused-argument
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """
        Execute an inference request.

        Args:
            prompt: The prompt to pass into the model.
            stop: A list of strings to stop generation when encountered

        Returns:
            The string generated by the model
        """
        text_callback = None
        if run_manager:
            text_callback = partial(run_manager.on_llm_new_token, verbose=self.verbose)

        invocation_params = self._get_model_default_parameters
        invocation_params.update(kwargs)
        invocation_params["prompt"] = [[prompt]]
        model_params = self._identifying_params
        model_params.update(kwargs)
        # request_id must be uint64_t otherwise triton will throw error
        request_id = str(random.getrandbits(64))

        self.client.load_model(model_params["model_name"])
        result_queue = self.client.request_streaming(
            model_params["model_name"], request_id, **invocation_params
        )

        response = ""
        for token in result_queue:
            if text_callback:
                text_callback(token)
            response = response + token
        return response


class StreamingResponseGenerator(queue.Queue[Optional[str]]):
    """A Generator that provides the inference results from an LLM."""

    def __init__(self, client: "GrpcTritonClient", request_id: str) -> None:
        """Instantiate the generator class."""
        super().__init__()
        self._client = client
        self.request_id = request_id

    def __iter__(self) -> "StreamingResponseGenerator":
        """Return self as a generator."""
        return self

    def __next__(self) -> str:
        """Return the next retrieved token."""
        val = self.get()
        if val is None or val in STOP_WORDS:
            self._stop_stream()
            raise StopIteration()
        return val

    def _stop_stream(self) -> None:
        """Drain and shutdown the Triton stream."""
        # FUTURE: cancel the stream instead of threading
        self._client.stop_stream("tensorrt_llm", self.request_id)


class GrpcTritonClient:
    """An abstraction of the connection to a triton inference server."""

    def __init__(self, server_url: str) -> None:
        """Initialize the client."""
        self._server_url = server_url
        self._client = grpcclient.InferenceServerClient(server_url)

    def load_model(self, model_name: str, timeout: int = 1000) -> None:
        """Load a model into the server."""
        if self._client.is_model_ready(model_name):
            return

        self._client.load_model(model_name)
        t0 = time.perf_counter()
        t1 = t0
        while not self._client.is_model_ready(model_name) and t1 - t0 < timeout:
            t1 = time.perf_counter()

        if not self._client.is_model_ready(model_name):
            raise RuntimeError(f"Failed to load {model_name} on Triton in {timeout}s")

    def get_model_list(self) -> List[str]:
        """Get a list of models loaded in the triton server."""
        res = self._client.get_model_repository_index(as_json=True)
        return [model["name"] for model in res["models"]]

    def get_model_concurrency(self, model_name: str, timeout: int = 1000) -> int:
        """Get the modle concurrency."""
        self.load_model(model_name, timeout)
        instances = self._client.get_model_config(model_name, as_json=True)["config"][
            "instance_group"
        ]
        return sum(instance["count"] * len(instance["gpus"]) for instance in instances)

    def prepare_stop_signals(self) -> List[grpcclient.InferInput]:
        """Prepare input data for a model inference with stop signals."""
        inputs = [
            grpcclient.InferInput("input_ids", [1, 1], "INT32"),
            grpcclient.InferInput("input_lengths", [1, 1], "INT32"),
            grpcclient.InferInput("request_output_len", [1, 1], "UINT32"),
            grpcclient.InferInput("stop", [1, 1], "BOOL"),
        ]
        inputs[0].set_data_from_numpy(np.empty([1, 1], dtype=np.int32))
        inputs[1].set_data_from_numpy(np.zeros([1, 1], dtype=np.int32))
        inputs[2].set_data_from_numpy(np.array([[0]], dtype=np.uint32))
        inputs[3].set_data_from_numpy(np.array([[True]], dtype="bool"))
        return inputs

    # pylint: disable=R0913
    def send_stop_signals(self, model_name: str, request_id: str) -> None:
        """Send stop signals to a model for streaming inference."""
        stop_inputs = self.prepare_stop_signals()
        self._client.async_stream_infer(
            model_name,
            stop_inputs,
            request_id=request_id,
            parameters={"Streaming": True},
        )

    @staticmethod
    def _process_result(result: Dict[str, str]) -> str:
        """Post-process the result from the server."""
        message = ModelInferResponse()
        generated_text: str = ""
        google.protobuf.json_format.Parse(json.dumps(result), message)
        infer_result = grpcclient.InferResult(message)
        np_res = infer_result.as_numpy("text_output")

        if np_res:
            if np_res.ndim == 2:
                generated_text = np_res[0, 0].decode()
            else:
                generated_text = np_res[0].decode()
        else:
            generated_text = ""

        return generated_text

    def _stream_callback(
        self,
        result_queue: queue.Queue[Union[Optional[Dict[str, str]], str]],
        result: Any,
        error: str,
    ) -> None:
        """Add streamed result to queue."""
        if error:
            result_queue.put(error)
        else:
            response_raw = result.get_response(as_json=True)
            if "outputs" in response_raw:
                # the very last response might have no output, just the final flag
                response = self._process_result(response_raw)

                if response in STOP_WORDS:
                    result_queue.put(None)
                else:
                    result_queue.put(response)

            if response_raw["parameters"]["triton_final_response"]["bool_param"]:
                # end of the generation
                result_queue.put(None)

    def _send_prompt_streaming(
        self,
        model_name: str,
        request_inputs: Any,
        request_outputs: Optional[Any],
        request_id: str,
        result_queue: StreamingResponseGenerator,
    ) -> None:
        """Send the prompt and start streaming the result."""
        self._client.start_stream(callback=partial(self._stream_callback, result_queue))
        self._client.async_stream_infer(
            model_name=model_name,
            inputs=request_inputs,
            outputs=request_outputs,
            request_id=request_id,
        )

    def request_streaming(
        self,
        model_name: str,
        request_id: str,
        **params: Any,
    ) -> StreamingResponseGenerator:
        """Request a streaming connection."""
        if not self._client.is_model_ready(model_name):
            raise RuntimeError("Cannot request streaming, model is not loaded")

        result_queue = StreamingResponseGenerator(self, request_id)
        inputs = self._generate_inputs(**params)
        outputs = self._generate_outputs()
        self._send_prompt_streaming(
            model_name, inputs, outputs, request_id, result_queue
        )
        return result_queue

    def stop_stream(self, model_name: str, request_id: str) -> None:
        """Close the streaming connection."""
        self.send_stop_signals(model_name, request_id)
        self._client.stop_stream()

    @staticmethod
    def _generate_outputs() -> List["grpcclient.InferRequestedOutput"]:
        """Generate the expected output structure."""
        return [grpcclient.InferRequestedOutput("text_output")]

    @staticmethod
    def _prepare_tensor(name: str, input_data: Any) -> "grpcclient.InferInput":
        """Prepare an input data structure."""
        t = grpcclient.InferInput(
            name, input_data.shape, np_to_triton_dtype(input_data.dtype)
        )
        t.set_data_from_numpy(input_data)
        return t

    @staticmethod
    def _generate_inputs(  # pylint: disable=too-many-arguments,too-many-locals
        prompt: str,
        tokens: int = 300,
        temperature: float = 1.0,
        top_k: float = 1,
        top_p: float = 0,
        beam_width: int = 1,
        repetition_penalty: float = 1,
        length_penalty: float = 1.0,
    ) -> List["grpcclient.InferInput"]:
        """Create the input for the triton inference server."""
        query = np.array(prompt).astype(object)
        request_output_len = np.array([tokens]).astype(np.uint32).reshape((1, -1))
        runtime_top_k = np.array([top_k]).astype(np.uint32).reshape((1, -1))
        runtime_top_p = np.array([top_p]).astype(np.float32).reshape((1, -1))
        temperature_array = np.array([temperature]).astype(np.float32).reshape((1, -1))
        len_penalty = np.array([length_penalty]).astype(np.float32).reshape((1, -1))
        repetition_penalty_array = (
            np.array([repetition_penalty]).astype(np.float32).reshape((1, -1))
        )
        random_seed = np.array([RANDOM_SEED]).astype(np.uint64).reshape((1, -1))
        beam_width_array = np.array([beam_width]).astype(np.uint32).reshape((1, -1))
        streaming_data = np.array([[True]], dtype=bool)

        inputs = [
            GrpcTritonClient._prepare_tensor("text_input", query),
            GrpcTritonClient._prepare_tensor("max_tokens", request_output_len),
            GrpcTritonClient._prepare_tensor("top_k", runtime_top_k),
            GrpcTritonClient._prepare_tensor("top_p", runtime_top_p),
            GrpcTritonClient._prepare_tensor("temperature", temperature_array),
            GrpcTritonClient._prepare_tensor("length_penalty", len_penalty),
            GrpcTritonClient._prepare_tensor(
                "repetition_penalty", repetition_penalty_array
            ),
            GrpcTritonClient._prepare_tensor("random_seed", random_seed),
            GrpcTritonClient._prepare_tensor("beam_width", beam_width_array),
            GrpcTritonClient._prepare_tensor("stream", streaming_data),
        ]
        return inputs
