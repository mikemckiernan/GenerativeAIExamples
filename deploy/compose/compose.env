# full path to the model store directory of downloaded model
# NOTE: This should be an absolute path and not relative path
export MODEL_DIRECTORY="/home/nvidia/llama-2-13b-chat_vLLAMA-2-13B-CHAT-4K-FP16/model-store"

export MODEL_NAME="Llama-2-13b-chat-hf"

# [OPTIONAL] the number of GPUs to make available to the inference server
# export INFERENCE_GPU_COUNT="all"

# [OPTIONAL] the base directory inside which all persistent volumes will be created
# export DOCKER_VOLUME_DIRECTORY="."

# [OPTIONAL] the config file for chain server w.r.t. pwd
export APP_CONFIG_FILE=/dev/null

# [OPTIONAL] full path to the model store directory storing the nemo embedding model
export EMBEDDING_MODEL_DIRECTORY="/home/nvidia/nv-embed-qa_v003-L4"

# [OPTIONAL] name of the nemo embedding model
export EMBEDDING_MODEL_NAME="NV-Embed-QA-003"
# parameters for PGVector, update this when using PGVector Vecotor store
# export POSTGRES_PASSWORD=password
# export POSTGRES_USER=postgres
# export POSTGRES_DB=api
