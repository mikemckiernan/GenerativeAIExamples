# full path to the local copy of the model weights
# NOTE: This should be an absolute path and not relative path
export MODEL_DIRECTORY="/home/nvidia/llama-2-13b-chat_vLLAMA-2-13B-CHAT-4K-FP16-1-A100.24.01/model-store"

# Fill this out if you dont have a GPU. Leave this empty if you have a local GPU
export NVIDIA_API_KEY="nvapi-*"

# the name of the model being used - only for displaying on frontend
export MODEL_NAME="Llama-2-13b-chat-hf"

# [OPTIONAL] the number of GPUs to make available to the inference server
# export INFERENCE_GPU_COUNT="all"

# [OPTIONAL] the base directory inside which all persistent volumes will be created
# export DOCKER_VOLUME_DIRECTORY="."

# parameters for PGVector, update this when using PGVector Vector store
export POSTGRES_PASSWORD=password
export POSTGRES_USER=postgres
export POSTGRES_DB=api

# Update this line when using an external PGVector Vector store
export POSTGRES_HOST_IP=pgvector
export POSTGRES_PORT_NUMBER=5432

# [OPTIONAL] full path to the model store directory storing the nemo embedding model
export EMBEDDING_MODEL_DIRECTORY="/home/nvidia/nv-embed-qa_v003"

# [OPTIONAL] name of the nemo embedding model
export EMBEDDING_MODEL_NAME="NV-Embed-QA"
export EMBEDDING_MODEL_CKPT_NAME="NV-Embed-QA-003.nemo"