# full path to the local copy of the model weights
# NOTE: This should be an absolute path and not relative path
export MODEL_DIRECTORY="/home/nvidia/llama-2-13b-chat_vLLAMA-2-13B-CHAT-4K-FP16-1-A100.24.01/model-store"

# the number of GPUs needed by nemollm inference ms to deploy the model
export NUM_GPU=1

# GPU id which nemo embedding ms will use
export EMBEDDING_MS_GPU_ID=2

# Fill this out if you dont have a GPU. Leave this empty if you have a local GPU
export NVIDIA_API_KEY="nvapi-*"

# the name of the model being used - only for displaying on frontend
export MODEL_NAME="Llama-2-13b-chat-hf"

# [OPTIONAL] the number of GPUs to make available to the inference server
# export INFERENCE_GPU_COUNT="all"

# [OPTIONAL] the base directory inside which all persistent volumes will be created
# export DOCKER_VOLUME_DIRECTORY="."

# parameters for PGVector, update this when using PGVector Vector store
export POSTGRES_PASSWORD=password
export POSTGRES_USER=postgres
export POSTGRES_DB=api

# Update this line when using an external PGVector Vector store
export POSTGRES_HOST_IP=pgvector
export POSTGRES_PORT_NUMBER=5432

# full path to the model store directory storing the nemo embedding model
export EMBEDDING_MODEL_DIRECTORY="/home/nvidia/nv-embed-qa_v4"

# name of the nemo embedding model
export EMBEDDING_MODEL_NAME="NV-Embed-QA"
export EMBEDDING_MODEL_CKPT_NAME="NV-Embed-QA-4.nemo"