services:
  llm:
    container_name: nemollm-inference-ms
    image: nvcr.io/ohlfw0olaadg/ea-participants/nemollm-inference-ms:24.01
    volumes:
    - ${MODEL_DIRECTORY:?please update the env file and source it before running}:/model-store
    command: nemollm_inference_ms --model ${MODEL_NAME:?please update the env file and source it before running} --openai_port 9999 --nemo_port 9998 --num_gpus=${NUM_GPU:-1}
    ports:
    - "8000:8000"
    - "9998:9998"
    - "9999:9999"
    expose:
    - "8000"
    - "9998"
    - "9999"
    shm_size: 20gb
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: ${INFERENCE_GPU_COUNT:-all}
              capabilities: [gpu]

  embeddings:
    container_name: nemo-retriever-embedding-ms
    image: nvcr.io/ohlfw0olaadg/ea-participants/nemo-retriever-embedding-microservice:24.01
    volumes:
    - ${EMBEDDING_MODEL_DIRECTORY:?please update the env file and source it before running}:/model-checkpoint-path
    command:  bin/web -p 9080 -c /model-checkpoint-path/${EMBEDDING_MODEL_CKPT_NAME} -g model_config_templates/${EMBEDDING_MODEL_NAME}_template.yaml
    ports:
    - "9080:9080"
    expose:
    - "9080"
    shm_size: 8gb
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              # count: ${INFERENCE_GPU_COUNT:-all}
              device_ids: ['${EMBEDDING_MS_GPU_ID:-0}']
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9080/v1/health/ready"]
      interval: 30s
      timeout: 20s
      retries: 3
      start_period: 10m

  query:
    container_name: rag-application-multimodal-chatbot
    image: nvcr.io/ohlfw0olaadg/ea-rag-examples/rag-application-multimodal-chatbot:0.4.0
    command: --port 8081 --host 0.0.0.0
    environment:
      APP_LLM_SERVERURL: "llm:9999"
      APP_LLM_MODELNAME: ${MODEL_NAME:?please update the env file and source it before running}
      APP_LLM_MODELENGINE: nemo-infer-openai
      APP_EMBEDDINGS_SERVERURL: "embeddings:9080"
      APP_EMBEDDINGS_MODELNAME: ${EMBEDDING_MODEL_NAME:?please update the env file and source it before running}
      APP_EMBEDDINGS_MODELENGINE: nemo-embed
      APP_TEXTSPLITTER_CHUNKSIZE: 2000
      APP_TEXTSPLITTER_CHUNKOVERLAP: 200
      APP_PROMPTS_CHATTEMPLATE: "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Please ensure that your responses are positive in nature."
      APP_PROMPTS_RAGTEMPLATE: "You are a helpful AI assistant named Envie. You will reply to questions only based on the context that you are provided. If something is out of context, you will refrain from replying and politely decline to respond to the user."
      COLLECTION_NAME: multimodal-rag
      NVIDIA_API_KEY: ${NVIDIA_API_KEY:-""}
    ports:
    - "8081:8081"
    expose:
    - "8081"
    shm_size: 5gb
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    depends_on:
      - "llm"
      - "embeddings"

  frontend:
    container_name: rag-playground
    image: nvcr.io/ohlfw0olaadg/ea-rag-examples/rag-playground:0.4.0
    command: --port 8090
    environment:
      APP_SERVERURL: http://query
      APP_SERVERPORT: 8081
      APP_MODELNAME: ${MODEL_NAME:-mixtral_8x7b}
      RIVA_API_URI: ${RIVA_API_URI:-''}
      RIVA_API_KEY: ${RIVA_API_KEY:-''}
      RIVA_FUNCTION_ID: ${RIVA_FUNCTION_ID:-''}
      TTS_SAMPLE_RATE: ${TTS_SAMPLE_RATE:-48000}
    ports:
    - "8090:8090"
    expose:
    - "8090"
    depends_on:
    - query

networks:
  default:
    name: nvidia-rag