services:
  llm:
    container_name: nemollm-inference-ms
    image: nvcr.io/ohlfw0olaadg/ea-participants/nemollm-inference-ms:24.01
    volumes:
    - ${MODEL_DIRECTORY:?please update the env file and source it before running}:/model-store
    command: nemollm_inference_ms --model ${MODEL_NAME:?please update the env file and source it before running} --openai_port 9999 --nemo_port 9998 --num_gpus=${NUM_GPU:-1}
    ports:
    - "8000:8000"
    - "9998:9998"
    - "9999:9999"
    expose:
    - "8000"
    - "9998"
    - "9999"
    shm_size: 20gb
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: ${INFERENCE_GPU_COUNT:-all}
              capabilities: [gpu]

  embeddings:
    container_name: nemo-retriever-embedding-ms
    image: nvcr.io/ohlfw0olaadg/ea-participants/nemo-retriever-embedding-microservice:24.02-rc4
    volumes:
    - ${EMBEDDING_MODEL_DIRECTORY:?please update the env file and source it before running}:/model-checkpoint-path
    command:  bin/web -p 9080 -c /model-checkpoint-path/${EMBEDDING_MODEL_CKPT_NAME} -g model_config_templates/${EMBEDDING_MODEL_NAME}_template.yaml
    ports:
    - "9080:9080"
    expose:
    - "9080"
    shm_size: 8gb
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              # count: ${INFERENCE_GPU_COUNT:-all}
              device_ids: ['${EMBEDDING_MS_GPU_ID:-0}']
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9080/v1/health/ready"]
      interval: 30s
      timeout: 20s
      retries: 3
      start_period: 10m

  chain-server:
    container_name: rag-application-multimodal-chatbot
    image: nvcr.io/ohlfw0olaadg/ea-rag-examples/rag-application-multimodal-chatbot:0.5.0-rc1
    command: --port 8081 --host 0.0.0.0
    environment:
      APP_LLM_SERVERURL: "llm:9999"
      APP_LLM_MODELNAME: ${MODEL_NAME:?please update the env file and source it before running}
      APP_LLM_MODELENGINE: nemo-infer-openai
      APP_EMBEDDINGS_SERVERURL: "embeddings:9080"
      APP_EMBEDDINGS_MODELNAME: ${EMBEDDING_MODEL_NAME:?please update the env file and source it before running}
      APP_EMBEDDINGS_MODELENGINE: nemo-embed
      APP_TEXTSPLITTER_CHUNKSIZE: 510
      APP_TEXTSPLITTER_CHUNKOVERLAP: 200
      APP_PROMPTS_CHATTEMPLATE: "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Please ensure that your responses are positive in nature."
      APP_PROMPTS_RAGTEMPLATE: "You are a helpful AI assistant named Envie. You will reply to questions only based on the context that you are provided. If something is out of context, you will refrain from replying and politely decline to respond to the user."
      APP_RETRIEVER_TOPK: 4
      APP_RETRIEVER_SCORETHRESHOLD: 0.25
      COLLECTION_NAME: multimodal_rag
      NVIDIA_API_KEY: ${NVIDIA_API_KEY:-""}
    ports:
    - "8081:8081"
    expose:
    - "8081"
    shm_size: 5gb
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    depends_on:
      - "llm"
      - "embeddings"

  playground-helper:
    container_name: playground-helper
    image: nvcr.io/ohlfw0olaadg/ea-rag-examples/rag-playground:0.5.0-rc1
    command: --port 8090
    environment:
      APP_SERVERURL: http://chain-server
      APP_SERVERPORT: 8081
      APP_MODELNAME: ${MODEL_NAME:-mixtral_8x7b}
      RIVA_API_URI: ${RIVA_API_URI:-''}
      RIVA_API_KEY: ${RIVA_API_KEY:-''}
      RIVA_FUNCTION_ID: ${RIVA_FUNCTION_ID:-''}
      TTS_SAMPLE_RATE: ${TTS_SAMPLE_RATE:-48000}
    ports:
    - "8090:8090"
    expose:
    - "8090"
    depends_on:
    - chain-server

  rag-playground:
    container_name: rag-playground
    image: gitlab-master.nvidia.com:5005/ngc/apps/ai-playground:latest
    ports:
    - "3001:3001"
    expose:
    - "3001"
    depends_on:
    - playground-helper

networks:
  default:
    name: nvidia-rag
